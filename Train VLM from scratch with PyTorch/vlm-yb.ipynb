{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://huggingface.co/datasets/uygarkurt/simple-image-captions","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:11.839582Z","iopub.execute_input":"2025-08-10T17:45:11.839813Z","iopub.status.idle":"2025-08-10T17:45:13.229077Z","shell.execute_reply.started":"2025-08-10T17:45:11.839791Z","shell.execute_reply":"2025-08-10T17:45:13.228420Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'simple-image-captions'...\nremote: Enumerating objects: 31, done.\u001b[K\nremote: Counting objects: 100% (28/28), done.\u001b[K\nremote: Compressing objects: 100% (28/28), done.\u001b[K\nremote: Total 31 (delta 5), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\nUnpacking objects: 100% (31/31), 7.47 KiB | 1.07 MiB/s, done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import base64\nimport io\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport torch\nimport torch.nn as nn\nfrom transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer\nfrom transformers import ViTConfig, ViTModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport random\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:13.230000Z","iopub.execute_input":"2025-08-10T17:45:13.230276Z","iopub.status.idle":"2025-08-10T17:45:44.939104Z","shell.execute_reply.started":"2025-08-10T17:45:13.230249Z","shell.execute_reply":"2025-08-10T17:45:44.938496Z"}},"outputs":[{"name":"stderr","text":"2025-08-10 17:45:33.054977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754847933.267793      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754847933.329409      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"SEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:44.940943Z","iopub.execute_input":"2025-08-10T17:45:44.941625Z","iopub.status.idle":"2025-08-10T17:45:44.949553Z","shell.execute_reply.started":"2025-08-10T17:45:44.941601Z","shell.execute_reply":"2025-08-10T17:45:44.948764Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"BATCH_SIZE = 16\nN_HIDDEN_LAYERS = 16\nMAX_LENGTH = 16\nEVAL_INTERVAL = 10\nLEARNING_RATE = 9e-4\nEPOCHS = 6\nN_EMBD = 128\nN_HEAD = 8\nN_LAYER = 8\nDROPOUT = 0.4\nIMG_SIZE = 96\nPATCH_SIZE = 16\nIMAGE_EMBED_DIM = 512\nN_CHANNELS = 3\nMAX_POSITION_EMBEDDINGS = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:44.950128Z","iopub.execute_input":"2025-08-10T17:45:44.950361Z","iopub.status.idle":"2025-08-10T17:45:45.010138Z","shell.execute_reply.started":"2025-08-10T17:45:44.950333Z","shell.execute_reply":"2025-08-10T17:45:45.009338Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntokenizer = LlamaTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:45.010955Z","iopub.execute_input":"2025-08-10T17:45:45.011188Z","iopub.status.idle":"2025-08-10T17:45:47.572988Z","shell.execute_reply.started":"2025-08-10T17:45:45.011169Z","shell.execute_reply":"2025-08-10T17:45:47.572371Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1fb5dd178434ddc9b7983d51a7fd15c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b02f439c496474c896fb12003d82035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b7c66cf76543358f7fd7da35684b43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458382a2ffd7432c93f9086bf698d95c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d2bffa9b8c4ee3ad9d77049f5b28ec"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"image_dir = '/kaggle/working/simple-image-captions/'  # Directory containing images\n\ndef image_file_to_base64(image_filename):\n    image_path = image_dir + image_filename\n    with open(image_path, 'rb') as img_file:\n        b64_str = base64.b64encode(img_file.read()).decode('utf-8')\n    return b64_str\n\ndf = pd.read_csv(image_dir + 'inputs.csv', sep=\";\").dropna(axis=1, how=\"all\")\ndf['b64string_images'] = df['file'].apply(image_file_to_base64)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:47.573682Z","iopub.execute_input":"2025-08-10T17:45:47.573905Z","iopub.status.idle":"2025-08-10T17:45:47.679564Z","shell.execute_reply.started":"2025-08-10T17:45:47.573887Z","shell.execute_reply":"2025-08-10T17:45:47.678527Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"            file                          caption  \\\n0        car.png                          red car   \n1  astronaut.png  astronaut in a white space suit   \n2         tv.png      black television on a table   \n3      horse.png              brown horse running   \n4       wine.png                      wine bottle   \n\n                                    b64string_images  \n0  iVBORw0KGgoAAAANSUhEUgAABgAAAAQACAIAAACoEwUVAA...  \n1  iVBORw0KGgoAAAANSUhEUgAABAAAAAYACAIAAABn4K39AA...  \n2  iVBORw0KGgoAAAANSUhEUgAABgAAAAQACAIAAACoEwUVAA...  \n3  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n4  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>caption</th>\n      <th>b64string_images</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>car.png</td>\n      <td>red car</td>\n      <td>iVBORw0KGgoAAAANSUhEUgAABgAAAAQACAIAAACoEwUVAA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>astronaut.png</td>\n      <td>astronaut in a white space suit</td>\n      <td>iVBORw0KGgoAAAANSUhEUgAABAAAAAYACAIAAABn4K39AA...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tv.png</td>\n      <td>black television on a table</td>\n      <td>iVBORw0KGgoAAAANSUhEUgAABgAAAAQACAIAAACoEwUVAA...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>horse.png</td>\n      <td>brown horse running</td>\n      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wine.png</td>\n      <td>wine bottle</td>\n      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"config = ViTConfig(\n    image_size=IMG_SIZE,\n    patch_size=PATCH_SIZE,\n    num_channels=N_CHANNELS,\n    hidden_size=IMAGE_EMBED_DIM,\n    num_attention_heads=N_HEAD,\n    num_hidden_layers=N_HIDDEN_LAYERS,\n    intermediate_size=4 * IMAGE_EMBED_DIM,\n    hidden_dropout_prob=DROPOUT,\n    attention_probs_dropout_prob=DROPOUT,\n)\n\ntestvit = ViTModel(config)\nvit_input = torch.zeros(BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE)\ntestvit_out = testvit(vit_input).last_hidden_state[:, 0] # Get the [CLS] token representation\ntestvit_out.shape # (BATCH_SIZE, IMAGE_EMBED_DIM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:47.680510Z","iopub.execute_input":"2025-08-10T17:45:47.681163Z","iopub.status.idle":"2025-08-10T17:45:49.169038Z","shell.execute_reply.started":"2025-08-10T17:45:47.681096Z","shell.execute_reply":"2025-08-10T17:45:49.168378Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 512])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class VisionLanguageModel(nn.Module):\n    def __init__(\n        self,\n        n_embed,\n        image_embed_dim,\n        vocab_size,\n        n_layer,\n        n_head,\n        img_size,\n        patch_size,\n        n_hidden_layers,\n        dropout,\n        pad_token_id,\n        max_position_embeddings,\n        n_channels,\n    ):\n        super().__init__()\n        vit_config = ViTConfig(\n            image_size=img_size,\n            patch_size=patch_size,\n            num_channels=n_channels,\n            hidden_size=image_embed_dim,\n            num_attention_heads=n_head,\n            num_hidden_layers=n_hidden_layers,\n            intermediate_size=4 * image_embed_dim,\n            hidden_dropout_prob=dropout,\n            attention_probs_dropout_prob=dropout,\n        )\n        self.vision_encoder = ViTModel(vit_config)\n        self.image_projector = nn.Linear(image_embed_dim, n_embed)\n\n        llama_config = LlamaConfig(\n            vocab_size=vocab_size,\n            hidden_size=n_embed,\n            num_hidden_layers=n_layer,\n            num_attention_heads=n_head,\n            max_position_embeddings=max_position_embeddings,\n            pad_token_id=int(pad_token_id),\n        )\n        self.llama = LlamaForCausalLM(llama_config)\n        self.llama = self.llama.to(dtype=torch.bfloat16)  # Move Llama to bfloat16\n\n    def forward(self, img_array, input_ids, targets=None):\n        # img_array: [BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE]\n        # input_ids: [BATCH_SIZE, MAX_LENGTH]\n        image_embeds = self.vision_encoder(img_array).last_hidden_state[:, 0]  # [BATCH_SIZE, IMAGE_EMBED_DIM]\n        image_embeds_proj = self.image_projector(image_embeds).to(dtype=torch.bfloat16)  # [BATCH_SIZE, N_EMBED]\n        image_embeds_proj = image_embeds_proj.unsqueeze(1) # [BATCH_SIZE, 1, N_EMBED]\n\n        text_embeds = self.llama.model.embed_tokens(input_ids).to(dtype=torch.bfloat16)  # [BATCH_SIZE, MAX_LENGTH, N_EMBED]\n\n        input_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)  # [BATCH_SIZE, MAX_LENGTH + 1, N_EMBED]\n\n        attention_mask = torch.ones(input_embeds.shape[:2], dtype=torch.long, device=input_embeds.device) # [BATCH_SIZE, MAX_LENGTH + 1]\n\n        if targets is not None:\n            #target: [BATCH_SIZE, MAX_LENGTH]\n            targets = torch.cat([torch.full((targets.size(0), 1), -100, dtype=targets.dtype, device=targets.device), targets], dim=1) # [BATCH_SIZE, MAX_LENGTH + 1]\n            outputs = self.llama(\n                inputs_embeds=input_embeds,\n                attention_mask=attention_mask,\n                labels=targets,\n            )\n            return outputs.logits, outputs.loss\n        else:\n            outputs = self.llama(\n                inputs_embeds=input_embeds,\n                attention_mask=attention_mask,\n            )\n            return outputs.logits\n\n    @torch.no_grad()\n    def generate(self, img_array, input_ids, max_new_tokens=20):\n        # img_array: [BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE]\n        # input_ids: [BATCH_SIZE, MAX_LENGTH]\n        image_embeds = self.vision_encoder(img_array).last_hidden_state[:, 0]\n        image_embeds_proj = self.image_projector(image_embeds).unsqueeze(1).to(dtype=torch.bfloat16)\n\n        input_embeds = self.llama.model.embed_tokens(input_ids).to(dtype=torch.bfloat16)\n        inputs_embeds = torch.cat([image_embeds_proj, input_embeds], dim=1)\n        attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=inputs_embeds.device)\n       \n        generated = self.llama.generate(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            max_new_tokens=max_new_tokens,\n            pad_token_id=self.llama.config.pad_token_id,\n            eos_token_id=self.llama.config.eos_token_id,\n        )\n        return generated\n\nmodel = VisionLanguageModel(\n        N_EMBD,\n        IMAGE_EMBED_DIM,\n        tokenizer.vocab_size,\n        N_LAYER,\n        N_HEAD,\n        IMG_SIZE,\n        PATCH_SIZE,\n        N_HIDDEN_LAYERS,\n        DROPOUT,\n        tokenizer.pad_token_id,\n        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n        n_channels=N_CHANNELS,\n)\nmodel.to(device)\n\ndummy_img = torch.randn(1, N_CHANNELS, IMG_SIZE, IMG_SIZE).to(device)\ndummy_idx = torch.randint(0, tokenizer.vocab_size, (1, MAX_LENGTH)).to(device)\noutput = model(dummy_img, dummy_idx)\nprint(output.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:49.171329Z","iopub.execute_input":"2025-08-10T17:45:49.171570Z","iopub.status.idle":"2025-08-10T17:45:52.049116Z","shell.execute_reply.started":"2025-08-10T17:45:49.171544Z","shell.execute_reply":"2025-08-10T17:45:52.048337Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 17, 32000])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"img_path = '/kaggle/working/simple-image-captions/ship.png'\nimage = Image.open(img_path).convert('RGB')\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\nimg_tensor = transform(image).unsqueeze(0).to(device)  # Shape: [BATCH_SIZE (1), N_CHANNELS, IMG_SIZE, IMG_SIZE]\n\nprompt = \"A photo of\" \ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n\nwith torch.no_grad():\n    generated_ids = model.generate(img_tensor, input_ids, max_new_tokens=30)\n    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"Generated description of the given picture:\")\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:52.049913Z","iopub.execute_input":"2025-08-10T17:45:52.050256Z","iopub.status.idle":"2025-08-10T17:45:52.619678Z","shell.execute_reply.started":"2025-08-10T17:45:52.050237Z","shell.execute_reply":"2025-08-10T17:45:52.618901Z"}},"outputs":[{"name":"stdout","text":"Generated description of the given picture:\n第 районеек------ topologicalvin adapt Dynamic CREATE___ films областиissonHeadlr había Holzcr языMainругcretdeveloper≤ maincircleatura back wojторы\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def base64_to_tensor(base64_str, img_size=96):\n    image = Image.open(io.BytesIO(base64.b64decode(base64_str)))\n    if image.mode != 'RGB':\n        image = image.convert('RGB')\n    transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    return transform(image).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:52.620513Z","iopub.execute_input":"2025-08-10T17:45:52.620804Z","iopub.status.idle":"2025-08-10T17:45:52.626107Z","shell.execute_reply.started":"2025-08-10T17:45:52.620780Z","shell.execute_reply":"2025-08-10T17:45:52.625331Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class VLMDataset(Dataset):\n    def __init__(self, df, img_size=96, tokenizer=None):\n        self.df = df.reset_index(drop=True)\n        self.img_size = img_size\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_b64 = self.df.loc[idx, 'b64string_images']\n        caption = self.df.loc[idx, 'caption']\n        image = base64_to_tensor(img_b64, self.img_size).squeeze(0)\n        encoding = self.tokenizer(\n            caption,\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=MAX_LENGTH\n        )\n        input_ids = encoding.input_ids.squeeze(0)\n        targets = input_ids.clone()\n        targets[:-1] = input_ids[1:]\n        targets[-1] = self.tokenizer.pad_token_id\n        return image, input_ids, targets\n\ndf = pd.concat([df] * 50)[['b64string_images', 'caption']]\nn = int(0.9 * len(df))\ndf_train = df.iloc[:n]\ndf_val = df.iloc[n:]\n\ntrain_dataset = VLMDataset(df_train, img_size=IMG_SIZE, tokenizer=tokenizer)\nval_dataset = VLMDataset(df_val, img_size=IMG_SIZE, tokenizer=tokenizer)\n\nprint(len(train_dataset))\nprint(len(train_dataset[0]))\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:52.626769Z","iopub.execute_input":"2025-08-10T17:45:52.627086Z","iopub.status.idle":"2025-08-10T17:45:52.730892Z","shell.execute_reply.started":"2025-08-10T17:45:52.627067Z","shell.execute_reply":"2025-08-10T17:45:52.730215Z"}},"outputs":[{"name":"stdout","text":"720\n3\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss(model, val_loader):\n    losses = []\n    model.eval()\n    for images, input_ids, targets in val_loader:\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        targets = targets.to(device)\n        _, loss = model(images, input_ids, targets)\n        losses.append(loss.item())\n    return sum(losses) / len(losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:52.731650Z","iopub.execute_input":"2025-08-10T17:45:52.731858Z","iopub.status.idle":"2025-08-10T17:45:52.736420Z","shell.execute_reply.started":"2025-08-10T17:45:52.731842Z","shell.execute_reply":"2025-08-10T17:45:52.735642Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs, learning_rate, eval_interval):\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    for epoch in range(epochs):\n        model.train()\n        loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batch_idx, (images, input_ids, targets) in loop:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            targets = targets.to(device)\n            optimizer.zero_grad()\n            logits, loss = model(images, input_ids, targets)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % eval_interval == 0:\n                loop.set_postfix(loss=loss.item())\n        val_loss = estimate_loss(model, val_loader)\n        print(f\"Validation Loss after epoch {epoch}: {val_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:52.737027Z","iopub.execute_input":"2025-08-10T17:45:52.737254Z","iopub.status.idle":"2025-08-10T17:45:52.756114Z","shell.execute_reply.started":"2025-08-10T17:45:52.737229Z","shell.execute_reply":"2025-08-10T17:45:52.755535Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, EVAL_INTERVAL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:45:52.756869Z","iopub.execute_input":"2025-08-10T17:45:52.757180Z","iopub.status.idle":"2025-08-10T17:47:42.116378Z","shell.execute_reply.started":"2025-08-10T17:45:52.757155Z","shell.execute_reply":"2025-08-10T17:47:42.115769Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/6: 100%|██████████| 45/45 [00:16<00:00,  2.70it/s, loss=2.51]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss after epoch 0: 2.300389528274536\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/6: 100%|██████████| 45/45 [00:16<00:00,  2.74it/s, loss=1.14]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss after epoch 1: 1.187171846628189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/6: 100%|██████████| 45/45 [00:16<00:00,  2.75it/s, loss=0.514]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss after epoch 2: 0.516754224896431\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/6: 100%|██████████| 45/45 [00:16<00:00,  2.75it/s, loss=0.3]  \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss after epoch 3: 0.2887508273124695\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/6: 100%|██████████| 45/45 [00:16<00:00,  2.73it/s, loss=0.225]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss after epoch 4: 0.2314356490969658\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/6: 100%|██████████| 45/45 [00:16<00:00,  2.75it/s, loss=0.174]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss after epoch 5: 0.15519629046320915\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"img_path = '/kaggle/working/simple-image-captions/guitar.png'\nimage = Image.open(img_path).convert('RGB')\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\nimg_tensor = transform(image).unsqueeze(0).to(device)\n\nprompt = \"A photo of\"\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    generated_ids = model.generate(img_tensor, input_ids, max_new_tokens=30)\n    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"Generated description of the given picture:\")\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:47:42.117248Z","iopub.execute_input":"2025-08-10T17:47:42.117502Z","iopub.status.idle":"2025-08-10T17:47:42.217700Z","shell.execute_reply.started":"2025-08-10T17:47:42.117476Z","shell.execute_reply":"2025-08-10T17:47:42.217187Z"}},"outputs":[{"name":"stdout","text":"Generated description of the given picture:\nmicrooust guitar aphon\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}